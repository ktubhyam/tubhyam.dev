---
title: "Spekron"
date: 2026-01-20
description: "A hybrid Mamba-Transformer foundation model for vibrational spectroscopy with wavelet embeddings, MoE routing, and VIB disentanglement."
tags: ["pytorch", "deep-learning", "spectroscopy", "foundation-model"]
status: "active"
github: "https://github.com/ktubhyam/Spekron"
featured: true
---

Spekron is a self-supervised foundation model for vibrational spectroscopy that achieves few-shot calibration transfer across instruments and modalities.

## Architecture

- **Wavelet Embedding** — Daubechies-4 DWT with learnable 1D CNN patching
- **Mamba Backbone** — Selective state-space models for O(n) sequence processing
- **Mixture of Experts** — Top-2 gating with optional KAN activations
- **Transformer Encoder** — Global attention for cross-position reasoning
- **VIB Head** — Disentangles chemistry (z_chem) from instrument signature (z_inst)

## Key Features

- Multi-task pretraining: masked reconstruction + contrastive + denoising
- LoRA-based fine-tuning for efficient transfer
- Test-Time Training for zero-shot instrument adaptation
- Physics-informed losses: Beer-Lambert, non-negativity, smoothness

## Training Infrastructure

- 4x RTX 5090 GPUs with DataParallel
- Mixed precision (AMP) training
- W&B experiment tracking
- 61K+ spectra pretraining corpus

## Related

- **Paper:** [Hybrid SSA Spectroscopy](/research/hybrid-ssa-spectroscopy) — the research paper describing Spekron's architecture and results
- **Theory:** [Spectral Identifiability](/research/spectral-identifiability) — information-theoretic framework motivating the VIB design
- **Preprocessing:** [SpectraKit](/projects/spectrakit) — the spectral preprocessing library powering Spekron's data pipeline
- **Blog:** [The Spectral Inverse Problem](/blog/spectral-inverse-problem) — accessible overview of the theory behind spectral inversion
