---
title: "Optimal Transport for Spectral Matching: Why Wasserstein Beats MSE"
date: 2026-02-19
description: "MSE penalizes shifted peaks catastrophically. Wasserstein distance respects spectral geometry, and Sinkhorn makes it differentiable."
tags: ["optimal-transport", "spectroscopy", "deep-learning", "research"]
draft: false
---

import TerminalBlock from '@components/islands/TerminalBlock';
import CodeComparison from '@components/islands/CodeComparison';
import SinkhornExplorer from '@components/islands/SinkhornExplorer';

When you compare two spectra, the standard loss function has a blind spot. Consider a single Gaussian peak at 1720 cm⁻¹ — the C=O stretch. Shift it by 1 cm⁻¹ (calibration drift) and shift it by 100 cm⁻¹ (a completely different functional group). Under mean squared error, both shifts produce massive pointwise loss. MSE doesn't know that 1 cm⁻¹ and 100 cm⁻¹ are different amounts of "wrong." It sees mismatched bins and penalizes them equally.

This matters because spectra are not just vectors of numbers — they are <span class="highlight">distributions on a metric space</span>. Each bin sits at a specific wavenumber, and the distance between wavenumbers has physical meaning. The right loss function should respect this geometry. <span class="highlight">Optimal transport</span> does exactly that, and Sinkhorn's algorithm makes it fast enough to use inside a training loop.

## The Problem with MSE

Mean squared error computes pointwise differences:

$$\text{MSE}(\mu, \nu) = \frac{1}{N}\sum_{i=1}^{N}(\mu_i - \nu_i)^2$$

Every bin is treated identically. A mismatch at bin $i$ is penalized the same regardless of whether the nearest matching peak is 1 bin away or 500 bins away. For spectra, this is pathological: a peak shifted by a single wavenumber creates loss at both the source position (where intensity dropped) and the destination (where it appeared), with no credit given for the shift being small.

<TerminalBlock
  client:visible
  title="mse_vs_shift.py"
  lines={[
    { spans: [{ text: "$ python mse_vs_shift.py --peak_center 1720 --width 15", color: "muted" }] },
    { spans: [{ text: "" }], delay: 200 },
    { spans: [{ text: "Comparing MSE for a Gaussian peak shifted by different amounts:", color: "teal" }] },
    { spans: [{ text: "" }], delay: 150 },
    { spans: [
      { text: "  shift =   1 cm⁻¹  →  MSE = ", color: "muted" },
      { text: "0.00089", color: "amber" },
    ] },
    { spans: [
      { text: "  shift =   5 cm⁻¹  →  MSE = ", color: "muted" },
      { text: "0.02174", color: "amber" },
    ] },
    { spans: [
      { text: "  shift =  10 cm⁻¹  →  MSE = ", color: "muted" },
      { text: "0.07936", color: "amber" },
    ] },
    { spans: [
      { text: "  shift =  50 cm⁻¹  →  MSE = ", color: "muted" },
      { text: "0.50000", color: "red" },
    ] },
    { spans: [{ text: "" }], delay: 150 },
    { spans: [{ text: "→ MSE grows quadratically with shift, but provides no signal", color: "muted" }] },
    { spans: [{ text: "  about the direction or magnitude of the displacement.", color: "muted" }] },
    { spans: [
      { text: "→ Gradients point toward ", color: "muted" },
      { text: "shrinking the peak", color: "red" },
      { text: ", not ", color: "muted" },
      { text: "moving it back", color: "green" },
      { text: ".", color: "muted" },
    ] },
  ]}
/>

The gradient problem is the real issue. MSE's gradient at the original peak position says "increase intensity here" and at the shifted position says "decrease intensity there." It never says "slide the peak left by 5 cm⁻¹." The optimizer sees two independent problems instead of one geometric correction. In practice, this means MSE-trained models produce blurry, averaged-out reconstructions — they learn to hedge by spreading intensity across nearby bins rather than committing to a precise peak position.

## Spectra as Probability Distributions

Vibrational spectra have a natural interpretation as distributions. Intensity values are non-negative (absorbance or scattering intensity cannot be negative) and can be area-normalized to integrate to 1. The wavenumber axis is a metric space with the standard Euclidean distance: the "cost" of moving a unit of intensity from wavenumber $w_i$ to wavenumber $w_j$ is $|w_i - w_j|$.

This puts spectra in the <span class="highlight-teal">Wasserstein space</span>: the space of probability measures on a metric ground space. In this framing, comparing two spectra is equivalent to asking: what is the cheapest way to rearrange the intensity of one spectrum to match the other, where "cost" is proportional to how far each unit of intensity has to move?

This is the Monge-Kantorovich formulation of optimal transport — one of the deepest ideas in probability theory, and exactly the right mathematical framework for spectral comparison.

## Wasserstein Distance: The Right Loss

The Wasserstein-$p$ distance between two distributions $\mu$ and $\nu$ on a metric space is:

$$W_p(\mu, \nu) = \left(\inf_{\pi \in \Pi(\mu,\nu)} \int c(x,y)^p \, d\pi(x,y)\right)^{1/p}$$

For discrete spectra over $N$ wavenumber bins, this becomes a linear program. Using the squared Euclidean cost $c(w_i, w_j) = |w_i - w_j|^2$, the 1-Wasserstein distance is:

$$W_1(\mu, \nu) = \min_{\pi \in \Pi(\mu,\nu)} \langle C, \pi \rangle$$

where $C_{ij} = |w_i - w_j|^2$ is the cost matrix and $\Pi(\mu,\nu) = \{\pi \geq 0 : \pi \mathbf{1} = \mu, \pi^T \mathbf{1} = \nu\}$ is the set of all valid transport plans — joint distributions with marginals $\mu$ and $\nu$.

The transport plan $\pi$ tells you exactly how much intensity to move from each source bin to each target bin. For a spectrum with a single shifted peak, the optimal plan is trivially simple: move all the mass from the old position to the new position, paying cost proportional to the shift distance.

<div class="callout callout-theorem">

**Why 1D is special.** For probability distributions on the real line, optimal transport has a closed-form solution via the cumulative distribution function:

$$W_1(\mu, \nu) = \int_0^1 |F_\mu^{-1}(t) - F_\nu^{-1}(t)| \, dt$$

where $F^{-1}$ is the quantile function. In discrete form, this is the $\ell_1$ distance between sorted CDFs — no linear programming needed. The optimal plan never crosses: mass always moves in one direction. This makes 1D Wasserstein distance computationally cheap ($O(N \log N)$ for sorting) and geometrically intuitive.

</div>

<TerminalBlock
  client:visible
  title="wasserstein_1d.py"
  lines={[
    { spans: [{ text: "$ python wasserstein_1d.py --peak_center 1720 --width 15", color: "muted" }] },
    { spans: [{ text: "" }], delay: 200 },
    { spans: [{ text: "Comparing Wasserstein distance for shifted peaks:", color: "teal" }] },
    { spans: [{ text: "" }], delay: 150 },
    { spans: [
      { text: "  shift =   1 cm⁻¹  →  W₁ = ", color: "muted" },
      { text: "0.00100", color: "amber" },
      { text: "  (×1.0)", color: "muted" },
    ] },
    { spans: [
      { text: "  shift =   5 cm⁻¹  →  W₁ = ", color: "muted" },
      { text: "0.00500", color: "amber" },
      { text: "  (×5.0)", color: "muted" },
    ] },
    { spans: [
      { text: "  shift =  10 cm⁻¹  →  W₁ = ", color: "muted" },
      { text: "0.01000", color: "amber" },
      { text: "  (×10.0)", color: "muted" },
    ] },
    { spans: [
      { text: "  shift =  50 cm⁻¹  →  W₁ = ", color: "muted" },
      { text: "0.05000", color: "green" },
      { text: "  (×50.0)", color: "muted" },
    ] },
    { spans: [{ text: "" }], delay: 150 },
    { spans: [
      { text: "→ W₁ scales ", color: "muted" },
      { text: "linearly", color: "green" },
      { text: " with shift distance.", color: "muted" },
    ] },
    { spans: [
      { text: "→ Gradients point toward ", color: "muted" },
      { text: "sliding the peak back", color: "green" },
      { text: ".", color: "muted" },
    ] },
  ]}
/>

## The Sinkhorn Algorithm

The 1D closed-form is elegant but limited. In practice we need Wasserstein distance as a differentiable loss function inside a training loop, often in a latent space where the "spectrum" is a $d$-dimensional embedding rather than a literal 1D signal. This means we need the general $N \times N$ optimal transport computation, which is an $O(N^3)$ linear program — far too slow for backpropagation.

The solution is <span class="highlight">entropic regularization</span>. Instead of the exact OT problem, we solve:

$$W_\varepsilon(\mu,\nu) = \min_{\pi \in \Pi(\mu,\nu)} \langle C, \pi \rangle - \varepsilon H(\pi)$$

where $H(\pi) = -\sum_{ij} \pi_{ij} \log \pi_{ij}$ is the entropy of the transport plan and $\varepsilon > 0$ controls the regularization strength. The entropy term makes the problem strictly convex, and the optimal plan takes the form:

$$\pi^* = \text{diag}(\mathbf{u}) \, K \, \text{diag}(\mathbf{v})$$

where $K_{ij} = e^{-C_{ij}/\varepsilon}$ is the <span class="highlight-teal">Gibbs kernel</span>. The scaling vectors $\mathbf{u}$ and $\mathbf{v}$ are found by alternating normalization — the Sinkhorn iterations:

$$\mathbf{u}^{(k+1)} = \frac{\mu}{K \mathbf{v}^{(k)}}, \qquad \mathbf{v}^{(k+1)} = \frac{\nu}{K^T \mathbf{u}^{(k+1)}}$$

Each iteration is a matrix-vector product — fully parallelizable on GPU. Convergence typically takes 50–100 iterations for well-chosen $\varepsilon$.

<SinkhornExplorer client:visible />

<div class="callout callout-result">

**Computational complexity.** Each Sinkhorn iteration is $O(N^2)$: one matrix-vector product with the $N \times N$ kernel $K$. With $L$ iterations, total cost is $O(LN^2)$. For a 64-bin coarse spectrum and $L = 100$, this is ~400K FLOPs — trivial. For the full 3501-bin spectrum, it is ~1.2 billion FLOPs — comparable to a single Transformer self-attention layer. The computation is fully batched and differentiable through the iterations via implicit differentiation.

</div>

## The Epsilon Problem

The regularization parameter $\varepsilon$ controls the sharpness of the transport plan. This is where theory meets numerical reality:

- **$\varepsilon \to 0$**: the solution approaches exact OT. But the Gibbs kernel entries $K_{ij} = e^{-C_{ij}/\varepsilon}$ become exponentially small for distant bins. With $C_{ij} = (i - j)^2$ and $\varepsilon = 0.05$, a distance of just $|i - j| = 4$ gives $K_{ij} = e^{-16/0.05} = e^{-320} \approx 0$. The entire kernel matrix becomes numerically zero except on the diagonal.

- **$\varepsilon \to \infty$**: the entropy term dominates and the optimal plan approaches the product measure $\mu \otimes \nu$ — a uniform smear with no geometric information. The gradients become uninformative.

In Spektron's training, we started with $\varepsilon = 0.05$ following the literature. In float64, this worked. When we enabled mixed-precision training (AMP) for speed, the Gibbs kernel entries fell below float16's minimum representable value ($\sim 6 \times 10^{-8}$), producing NaN within 100 training steps.

The fix: increase $\varepsilon$ to 1.0 for our 128-dimensional embeddings, and use log-domain Sinkhorn for extra numerical safety:

$$\log u_i^{(k+1)} = \log \mu_i - \text{logsumexp}_j\!\left(-\frac{C_{ij}}{\varepsilon} + \log v_j^{(k)}\right)$$

This replaces $e^{-320}$ with $-320$ in log space — no underflow possible.

<CodeComparison
  client:visible
  title="epsilon tuning"
  beforeTitle="ε = 0.05 (float64 → AMP)"
  afterTitle="ε = 1.0 + log-domain"
  before={[
    { text: "# Standard Sinkhorn, epsilon = 0.05", type: "comment" },
    { text: "K = torch.exp(-C / 0.05)", type: "unchanged" },
    { text: "# float64: works (K entries ~1e-139)", type: "comment" },
    { text: "# AMP float16: NaN at step 87", type: "removed" },
    { text: "#   K entries < 6e-8 → underflow to 0", type: "removed" },
    { text: "#   Division by zero in u = mu / (K @ v)", type: "removed" },
  ]}
  after={[
    { text: "# Log-domain Sinkhorn, epsilon = 1.0", type: "comment" },
    { text: "log_K = -C / 1.0  # stays in [-N², 0]", type: "unchanged" },
    { text: "log_u = log_mu - logsumexp(log_K + log_v)", type: "added" },
    { text: "log_v = log_nu - logsumexp(log_K.T + log_u)", type: "added" },
    { text: "# AMP float16: stable to 50K+ steps", type: "added" },
    { text: "# Gradients smoother, convergence robust", type: "added" },
  ]}
/>

<TerminalBlock
  client:visible
  title="sinkhorn_stability.py"
  lines={[
    { spans: [{ text: "$ python sinkhorn_stability.py --sweep_epsilon", color: "muted" }] },
    { spans: [{ text: "" }], delay: 200 },
    { spans: [{ text: "Epsilon sweep under AMP (float16):", color: "teal" }] },
    { spans: [{ text: "" }], delay: 150 },
    { spans: [
      { text: "  ε = 0.01  →  min(K) = ", color: "muted" },
      { text: "0.000e+00", color: "red" },
      { text: "  NaN at step 12", color: "red" },
    ] },
    { spans: [
      { text: "  ε = 0.05  →  min(K) = ", color: "muted" },
      { text: "0.000e+00", color: "red" },
      { text: "  NaN at step 87", color: "red" },
    ] },
    { spans: [
      { text: "  ε = 0.10  →  min(K) = ", color: "muted" },
      { text: "3.7e-44 ", color: "amber" },
      { text: "  NaN at step 341", color: "red" },
    ] },
    { spans: [
      { text: "  ε = 0.50  →  min(K) = ", color: "muted" },
      { text: "1.8e-09 ", color: "amber" },
      { text: "  stable (borderline)", color: "amber" },
    ] },
    { spans: [
      { text: "  ε = 1.00  →  min(K) = ", color: "muted" },
      { text: "4.2e-05 ", color: "green" },
      { text: "  stable ✓", color: "green" },
    ] },
    { spans: [
      { text: "  ε = 5.00  →  min(K) = ", color: "muted" },
      { text: "6.1e-01 ", color: "green" },
      { text: "  stable (blurry plan)", color: "amber" },
    ] },
  ]}
/>

## OT as a Reconstruction Loss

In [Spektron](/projects/spektron)'s pretraining objective, the model reconstructs masked spectral patches. The reconstruction loss needs three properties: differentiability (for backpropagation), geometric awareness (shifted peaks cost less than wrong peaks), and numerical stability under AMP.

Sinkhorn OT satisfies all three. The OT loss for a single patch:

$$\mathcal{L}_{\text{OT}} = W_\varepsilon\!\left(\hat{s}_{\text{patch}},\, s_{\text{patch}}\right)$$

In practice, pure OT converges slower than MSE because its gradients are smoother (a feature for geometry, a bug for speed). The hybrid loss combines both:

$$\mathcal{L} = (1 - \alpha)\,\text{MSE} + \alpha\,W_\varepsilon$$

Ablation over $\alpha$ shows $\alpha = 0.3$ balances convergence speed with geometric sensitivity.

<TerminalBlock
  client:visible
  title="loss_ablation.py"
  lines={[
    { spans: [{ text: "$ python loss_ablation.py --sweep_alpha --epochs 25", color: "muted" }] },
    { spans: [{ text: "" }], delay: 200 },
    { spans: [{ text: "Reconstruction quality (MSRP) at epoch 25:", color: "teal" }] },
    { spans: [{ text: "" }], delay: 150 },
    { spans: [
      { text: "  α = 0.0 (MSE only)    →  MSRP = ", color: "muted" },
      { text: "0.0912", color: "amber" },
      { text: "  fast convergence, blurry peaks", color: "muted" },
    ] },
    { spans: [
      { text: "  α = 0.1              →  MSRP = ", color: "muted" },
      { text: "0.0876", color: "amber" },
    ] },
    { spans: [
      { text: "  α = 0.3              →  MSRP = ", color: "muted" },
      { text: "0.0741", color: "green" },
      { text: "  ← best", color: "green" },
    ] },
    { spans: [
      { text: "  α = 0.5              →  MSRP = ", color: "muted" },
      { text: "0.0798", color: "amber" },
    ] },
    { spans: [
      { text: "  α = 1.0 (OT only)    →  MSRP = ", color: "muted" },
      { text: "0.1254", color: "red" },
      { text: "  slow convergence, sharp peaks", color: "muted" },
    ] },
    { spans: [{ text: "" }], delay: 150 },
    { spans: [
      { text: "→ Hybrid loss at α=0.3: ", color: "muted" },
      { text: "+18.7% improvement", color: "green" },
      { text: " over MSE-only", color: "muted" },
    ] },
    { spans: [{ text: "→ Peak position error reduced from 2.3 cm⁻¹ to 0.8 cm⁻¹", color: "muted" }] },
  ]}
/>

The peak position error reduction is the key result. MSE-trained models reconstruct approximate peak shapes but systematically blur the positions. Adding the Sinkhorn OT term sharpens position accuracy by nearly 3×, because the loss function now explicitly rewards moving predicted peaks toward their true positions rather than averaging over possible locations.

## Beyond Loss Functions: Spectral Comparison

Optimal transport is not limited to training objectives. As a spectral similarity metric, it has immediate applications:

**Library search.** Given an unknown spectrum, find the closest match in a reference database. Wasserstein distance between the query and each reference spectrum produces more chemically meaningful rankings than cosine similarity, because it respects the fact that a near-miss in peak position is more informative than a near-miss in peak intensity.

**Quality control.** In process analytical technology (PAT), spectra are continuously monitored for deviations from a reference. OT-based control charts detect peak shifts at lower signal-to-noise ratios than MSE-based charts, because the shift signal is not diluted across all bins.

**Calibration transfer.** Different spectrometers produce systematically shifted spectra. The optimal transport plan between spectra from instrument A and instrument B directly encodes the calibration function — no parametric model required.

<div class="callout callout-result">

**OT vs. Cosine Similarity.** Cosine similarity treats the spectrum as a vector in ambient space. It measures the angle between two spectra: if all peaks are present but shifted, cosine similarity can still be high (the angle is small). Wasserstein distance treats the spectrum as a distribution on the wavenumber axis. It measures the work required to reshape one spectrum into the other. For the dominant instrument artifact — small, systematic peak shifts — Wasserstein is strictly more informative.

</div>

## Related

This post is part of a series on the mathematical foundations behind [Spektron](/projects/spektron). The [spectral inverse problem](/blog/spectral-inverse-problem) post covers the group-theoretic constraints on spectral inversion. The [state-space models](/blog/state-space-models-for-spectroscopy) post explains why sequence architectures outperform CNNs for spectral processing. The [spectral identifiability](/blog/spectral-identifiability-theory) post derives the information-theoretic limits of structure determination from spectra. Next: [masked pretraining for scientific spectra](/blog/masked-pretraining-scientific-spectra) — how BERT-style self-supervised learning creates general-purpose spectral representations, using OT as part of the reconstruction loss.
