---
title: "Spektron"
date: 2026-01-20
description: "Benchmarking oscillatory SSMs for vibrational spectroscopy. D-LinOSS backbone, 5-architecture comparison, cross-spectral IR/Raman prediction, transfer function interpretability."
tags: ["pytorch", "deep-learning", "spectroscopy", "state-space-models"]
status: "active"
github: "https://github.com/ktubhyam/Spektron"
featured: true
---

Spektron is an empirical study evaluating whether oscillatory state-space models bring unique advantages to vibrational spectroscopy. It benchmarks five architectures (D-LinOSS, Mamba, Transformer, 1D CNN, S4D) on masked spectral reconstruction and cross-spectral prediction (IR to Raman), with transfer function analysis providing SSM-unique interpretability. Targeting publication at Digital Discovery (RSC).

## The Problem

State-space models have driven breakthroughs in NLP and genomics, but vibrational spectroscopy has zero prior SSM evaluations. This is surprising because spectral signals arise from molecular vibrations — damped harmonic oscillators — and D-LinOSS models exactly these dynamics. We ask: does this physics alignment translate to practical advantages?

Additionally, no prior work exists for cross-spectral prediction (predicting Raman from IR or vice versa), despite paired DFT datasets being readily available. Spektron introduces both the benchmark and the task.

## Architecture

The model uses a multi-stage encoder with seven loss objectives for self-supervised pretraining:

```
Raw Spectrum (B, 2048)
    → Raw Spectral Embedding (Conv1d, k=15, stride=1)
    → [CLS] + [DOMAIN] token prepend
    → D-LinOSS Backbone (4 layers, d_model=256, d_state=128)
    → Mixture of Experts (4 experts, top-k=2)
    → Transformer Encoder (2 blocks, 8 heads)
    → VIB Head → z_chem (128-dim) + z_inst (64-dim)
    → Task Heads: reconstruction | regression | transfer
```

### D-LinOSS Backbone

The backbone uses **Damped Linear Oscillatory State-Space** (D-LinOSS) layers instead of standard SSMs like Mamba. Each layer models a set of 128 damped harmonic oscillators — a physics-aligned inductive bias for spectroscopy, since vibrational spectra literally arise from molecular vibrations (harmonic and anharmonic oscillators).

Each oscillator is parameterized by:
- **Natural frequency** $\omega_i$ — learned diagonal entries of the stiffness matrix
- **Damping coefficient** $\gamma_i$ — controls energy dissipation
- **IMEX symplectic discretization** — preserves oscillatory structure during the 2048-step recurrence

The recurrence matrix for each oscillator takes the form:

$$M = \begin{pmatrix} 1 & \Delta t \\ -\Delta t^2 \cdot \omega^2 / S & 1 - \Delta t^2 \cdot \omega^2 / S \end{pmatrix}$$

where $S$ is the stride and $\Delta t$ is the step size. A critical stability requirement is the **CFL condition**: the ratio $\alpha = \Delta t^2 \cdot \omega^2 / S$ must remain below 2.0 to keep eigenvalues inside the unit circle. During training, $\omega$ values can grow, causing $\alpha$ to exceed this threshold and producing exponential divergence in the 2048-step scan. We clamp $\alpha \leq 1.99$ to guarantee stability.

The entire LinOSSBlock is forced to run in **float32** even under mixed precision, because the SSM scan accumulates values that can reach $\pm 200\text{K}$ — well beyond float16 range.

### Raw Spectral Embedding

Unlike the wavelet-based embedding used with the Mamba backbone, D-LinOSS operates on **full-resolution spectra** (2048 tokens, one per spectral point). A local Conv1d with kernel size 15 and stride 1 maps each spectral point to the model dimension, preserving all spectral detail. Wavenumber-aware positional encoding injects the physical frequency axis into the representation.

### Variational Information Bottleneck (VIB)

The VIB head splits the CLS token representation into two latent variables:
- **$z_\text{chem}$** (128-dim): chemistry-invariant representation, regularized via KL divergence
- **$z_\text{inst}$** (64-dim): instrument-specific information, trained to be discardable

A **gradient reversal layer** on $z_\text{chem}$ ensures it cannot encode instrument identity — the adversarial classifier receives reversed gradients, forcing $z_\text{chem}$ to be instrument-agnostic. This is the key mechanism enabling zero-shot transfer: at inference time, $z_\text{inst}$ is discarded and only $z_\text{chem}$ is used for downstream tasks.

### Mixture of Experts (MoE)

Four expert networks (one per modality: NIR, IR, Raman, Cross-domain) with top-k=2 sparse gating. Optional KAN (Kolmogorov-Arnold Network) activations in the expert FFNs provide interpretability — the learned activation shapes can reveal which spectral features each expert specializes in.

## Pretraining Objectives

Spektron uses seven concurrent loss functions during self-supervised pretraining:

| Loss | Weight | Purpose |
|------|--------|---------|
| **MSRP** (Masked Spectrum Reconstruction) | 1.0 | Mask 20% of spectral points in contiguous 3-point blocks, reconstruct from context |
| **Contrastive** (BYOL-style) | 0.3 | Same sample across instruments → similar $z_\text{chem}$ |
| **Denoising** | 0.2 | Reconstruct clean spectrum from augmented (noise, baseline drift, wavelength shift) input |
| **Physics-informed** | 0.1 | Beer-Lambert linearity, smoothness, non-negativity, peak symmetry |
| **Optimal Transport** (Sinkhorn) | 0.1 | Align latent distributions across instruments via Wasserstein distance |
| **VIB** | 0.15 | KL regularization + adversarial instrument classification with gradient reversal |
| **MoE balance** | 0.01 | Prevent expert collapse via load balancing |

A learnable `mask_token` parameter replaces masked positions in embedding space *before* the backbone — this is critical. Without masking the input, the model degenerates to a near-identity mapping (MSRP loss drops to 0.003 within 700 steps, learning nothing useful).

## Training Infrastructure

- **Hardware:** 2x NVIDIA RTX 5060 Ti (16GB each) via Vast.ai
- **Batch size:** 16 (8 per GPU), gradient accumulation 4 steps → effective batch 64
- **Optimizer:** AdamW (lr=3e-4, weight_decay=0.01) with linear warmup (1K steps) → cosine annealing
- **Precision:** bfloat16 AMP with LinOSSBlock forced to float32
- **Data:** 222K QM9S training samples (IR + Raman spectra, 2048 points each)
- **Throughput:** ~39 samples/sec, ~23 hours for 50K pretraining steps
- **Memory:** ~7.5GB per GPU

## Key Design Decisions

1. **D-LinOSS over Mamba:** The oscillatory dynamics of D-LinOSS naturally align with vibrational spectroscopy — each latent oscillator can learn to track a specific molecular vibration mode. Mamba's selective gating is more general but lacks this physics prior.

2. **Full-resolution embedding:** With D-LinOSS's O(n) complexity, we can process all 2048 spectral points as individual tokens (vs. 127 patches with wavelet embedding). No information is lost to patching.

3. **Gradient reversal for VIB:** Rather than training a separate adversarial loop, a gradient reversal layer in the forward pass cleanly separates instrument information from chemistry during backpropagation.

4. **CFL clamping:** Learned frequency parameters $\omega$ can grow unboundedly during training, causing the discretized recurrence to become unstable. Clamping the CFL ratio at 1.99 prevents eigenvalue escape without constraining the learned dynamics.

5. **Sinkhorn regularization at 1.0:** For 128-dimensional embeddings, the standard regularization (0.05) causes the transport kernel $\exp(-C/\epsilon)$ to underflow in float16. A regularization of 1.0 keeps the computation numerically stable.

## Four Experiments

| Experiment | Description |
|-----------|-------------|
| **E1: Architecture Benchmark** | D-LinOSS vs Mamba vs Transformer vs CNN vs S4D, all param-matched at ~2M backbone params. Masked spectral reconstruction on QM9S (130K molecules). 3 seeds, mean $\pm$ std. |
| **E2: Cross-Spectral Prediction** | Predict Raman from IR (and vice versa). The novel contribution — zero prior work. 5 metrics + identity-copy baseline. |
| **E3: Transfer Function Analysis** | H(z) filter bank visualization for trained D-LinOSS. Pole-zero plots, per-layer frequency coverage, trained vs random comparison. SSM-unique interpretability. |
| **E4: Calibration Transfer** | Fine-tune on Corn (80 $\times$ 3 NIR instruments) with 5-50 transfer samples. Compare against PDS, SBC, DS, CCA, di-PLS. |

## Related

- **Paper:** [Oscillatory SSMs for Spectroscopy](/research/hybrid-ssa-spectroscopy) — the research paper with benchmarks, cross-spectral prediction, and interpretability
- **Theory:** [Spectral Identifiability](/research/spectral-identifiability) — information-theoretic framework motivating the VIB design
- **Preprocessing:** [SpectraKit](/projects/spectrakit) — the spectral preprocessing library powering Spektron's data pipeline
- **Blog:** [The Spectral Inverse Problem](/blog/spectral-inverse-problem) — accessible overview of the theory behind spectral inversion
- **Blog:** [Masked Pretraining for Scientific Spectra](/blog/masked-pretraining-scientific-spectra) — lessons learned from the masking strategy
- **Blog:** [State-Space Models for Spectroscopy](/blog/state-space-models-for-spectroscopy) — why SSMs are a natural fit for spectral sequences
